{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"a01_intro/","title":"Class notes from Andrej Karpathy's Video Series on Chat GPT","text":""},{"location":"a01_intro/#links-code","title":"Links + Code","text":"<p>Youtube link: https://www.youtube.com/watch?v=kCc8FmEb1nY</p> <p>The code base is already in a repository called:</p> <ul> <li> <p>For the more simplistic code with step-by-step https://github.com/karpathy/ng-video-lecture</p> </li> <li> <p>For the code-complete w/training additions : https://github.com/karpathy/nanoGPT</p> </li> </ul>"},{"location":"a01_intro/#what-is-a-language-model","title":"What is a Language Model?","text":"<p>Models a sequence of words, and in its frame of reference it is completing a sequence of words.</p>"},{"location":"a01_intro/#what-is-the-neural-network-underlying-chat-gpt","title":"What is the neural network underlying chat GPT?","text":"<p>It comes from the paper <code>Attention is All you Need</code> paper from 2017. https://arxiv.org/abs/1706.03762</p> <ul> <li>Landmark paper that proposed the transformer architecture</li> </ul>"},{"location":"a01_intro/#what-does-gpt-stand-for","title":"What does GPT stand for?","text":"<p>GPT stands for Generatively, Pretrained Transformer. Transformer is the neural network architecture. Transformers were used widely in the next few years. </p> <ul> <li>GPT is trained on a large part of the internet</li> <li>GPT has a lot of fine tuning involved in it</li> </ul>"},{"location":"a01_intro/#what-we-will-do-together-instead-is-make-a-character-level-model-will-cover-the-differences-later","title":"What we will do together instead is make a character level model (will cover the differences later)","text":"<ul> <li> <p>Data source: tiny shakespeare</p> <ul> <li>all of shakespear concatenated together</li> <li>tensorflow: <code>https://www.tensorflow.org/datasets/catalog/tiny_shakespeare</code></li> <li>Model how all these characters follow each other</li> </ul> </li> <li> <p>problem setup:</p> <ul> <li>given a chunk of these characters + some context in the past</li> <li>will try and predict the next character that will come next</li> <li>can generate infinite shakespeare-like language</li> </ul> </li> </ul>"},{"location":"a01_intro/#contributing","title":"Contributing:","text":"<p>Easiest way to run the local site is:</p> <pre><code>mkdocs serve\n\n# INFO     -  Documentation built in 0.89 seconds\n# INFO     -  [20:53:27] Watching paths for changes: 'notes', 'mkdocs.yml'\n# INFO     -  [20:53:27] Serving on http://127.0.0.1:8000/\n</code></pre> <p>Then to publish to github</p> <pre><code>mkdocs gh-deploy\n</code></pre>"},{"location":"a02_text/","title":"Text Basics","text":"<p>How a machine understands text</p>"},{"location":"a02_text/#requirements","title":"Requirements","text":"<ul> <li><code>pytorch</code></li> <li><code>mlp</code></li> <li><code>python</code></li> </ul>"},{"location":"a02_text/#getting-started-with-transformers","title":"Getting started with transformers","text":"<p>There's a google colab that can be used as reference: Colab Link</p>"},{"location":"a02_text/#get-the-data","title":"Get the data","text":"<pre><code>!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n</code></pre>"},{"location":"a02_text/#opening-the-data","title":"Opening the data","text":"<p>Open the text file as a really long single string variable</p> <pre><code>def read_in_shakespear_data(file: str) -&gt; str:\n\"\"\"reads the simple text file from disk as a long string\"\"\"\nwith open(file, \"r\", encoding=\"utf-8\") as f:\nreturn f.read()\nfile = \"./notes/data/input.txt\"\nbigtext = read_in_shakespear_data(file)\n</code></pre> <p>Long is our source text?</p> <pre><code>print(\"total length of characters: {:,}\".format(len(bigtext)))\n# &gt;&gt;&gt; total length of characters: 1,115,394\n</code></pre> <p>What is a sample of the text?</p> <pre><code>print(bigtext[:1000])\n</code></pre> <pre><code>First Citizen:\nBefore we proceed any further, hear me speak.\n\nAll:\nSpeak, speak.\n\nFirst Citizen:\nYou are all resolved rather to die than to famish?\n\nAll:\nResolved. resolved.\n\nFirst Citizen:\nFirst, you know Caius Marcius is chief enemy to the people.\n\nAll:\nWe know't, we know't.\n\nFirst Citizen:\nLet us kill him, and we'll have corn at our own price.\nIs't a verdict?\n\nAll:\nNo more talking on't; let it be done: away, away!\n\nSecond Citizen:\nOne word, good citizens.\n\nFirst Citizen:\nWe are accounted poor citizens, the patricians good.\nWhat authority surfeits on would relieve us: if they\nwould yield us but the superfluity, while it were\nwholesome, we might guess they relieved us humanely;\nbut they think we are too dear: the leanness that\nafflicts us, the object of our misery, is as an\ninventory to particularise their abundance; our\nsufferance is a gain to them Let us revenge this with\nour pikes, ere we become rakes: for the gods know I\nspeak this in hunger for bread, not in thirst for revenge.\n</code></pre> <p>What are the unique characters found in the text?</p> <pre><code>unique_characters = sorted(list(set(bigtext)))\nprint(\"\".join(unique_characters))\n</code></pre> <pre><code>&gt;&gt;&gt;  !$&amp;',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n</code></pre> <p>Should also note that <code>newline</code> or <code>\\n</code> is also one of those characters, but cannot be visually seen because of the <code>print</code></p> <p>How many unique characters are there? </p> <pre><code>print(len(unique_characters))\n&gt;&gt;&gt; 65\n</code></pre> <p>So for the task we are developing, at any given time there are 65 possible choices to select for the next word</p>"},{"location":"a02_text/#tokenizing-the-data","title":"Tokenizing the data","text":"<p><code>tokenizing</code> refers to converting text into some series of numbers. There's mulitiple methods, libraries + approaches for doing this conversion. </p> <ul> <li>tokenizing = translating text -&gt; numbers</li> </ul> <p>Our approach here, will be to assign numbers to each unique charater. And then any word can be converted to a series of integers.</p> <pre><code>from typing import Sequence\nencoder = {char: j for j, char in enumerate(unique_characters)}\ndecoder = {j: char for j, char in enumerate(unique_characters)}\ndef encode(some_str: str) -&gt; Sequence[int]:\nreturn [encoder[char] for char in some_str]\ndef decode(sequence: Sequence[int]) -&gt; str:\nchars = [decoder[num] for num in sequence]\nreturn \"\".join(chars)\n</code></pre> <p>An example of encoding:</p> <pre><code>encode(\"ABCabc\")\n&gt;&gt;&gt; [13, 14, 15, 39, 40, 41]\n</code></pre> <p>A couple of observations: - since the encoder is sorted in abc order, similar characters are only a 1 step away - uppercase vs. lowercase will be treated differently in this model</p> <p>An example of decoding:</p> <pre><code>decode([0, 1, 2, 3])\n</code></pre> <pre><code>&gt;&gt;&gt; '\\n !$'\n</code></pre> <p>A few other examples</p> <pre><code>encode(\"my name is bob\")\n&gt;&gt;&gt; [51, 63, 1, 52, 39, 51, 43, 1, 47, 57, 1, 40, 53, 40]\n</code></pre> <pre><code>decode([51, 63, 1, 52, 39, 51, 43, 1, 47, 57, 1, 40, 53, 40])\n&gt;&gt;&gt; 'my name is bob'\n</code></pre>"},{"location":"a02_text/#other-approaches","title":"Other approaches","text":"<p><code>SentencePiece</code> that google uses a different vocabulary and a different approach. This is a <code>sub-word</code> tokenizer.</p>"},{"location":"a02_text/#what-does-sub-word-mean","title":"What does sub-word mean?","text":"<p>It means you are not encoding whole words, but at the same time not encoding characters either. <code>Subword</code> is pretty typical in practice. A quick example:</p> <pre><code>orignal = \"rerunning\"\nsubwork = \"re#\", \"run\", \"#ing\"\n</code></pre> <p>The above is an example of breaking up a word into <code>prefix</code>, <code>suffix</code>, and core <code>word</code> parts.</p> <p>Google's Encoder: Sentence Piece Github</p> <p>OpenAI's Encoder: TikToken Github</p>"},{"location":"a02_text/#example-from-openais-tokenizer","title":"Example from OpenAI's tokenizer:","text":"<pre><code>import tiktoken\nencoder = tiktoken.get_encoding(\"gpt2\")\nencoder.encode(\"my name is bob\")\n# &gt;&gt;&gt; [1820, 1438, 318, 29202]\nprint(encoder.n_vocab)\n# &gt;&gt;&gt; 50257\n</code></pre> <p>Random: - its funny that <code>bob</code> has its own token, but maybe thats more for \"something the water bobs up and down\" - the vocabulary is a LOT larger</p>"},{"location":"a02_text/#tokenize-the-entire-dataset","title":"Tokenize the entire dataset:","text":"<pre><code>import torch\ndef tokenize_shakespear(bigtext: str) -&gt; Sequence[int]:\ntoken_ids = torch.tensor(\nencode(bigtext),\ndtype=torch.long,\n)\nreturn token_ids\ntoken_ids = tokenize_shakespear(bigtext)\nprint(token_ids.shape, token_ids.dtype)\nprint(token_ids[:100])\n</code></pre> <p>A few general comments:</p> <p><code>encode(text)</code> - encodes the text into integers <code>torch.tensor(encode(text))</code> - then creates a large <code>long</code> 1D array to store the data and the data. FYI <code>long</code> is another way of saying <code>big integer</code>.</p> <pre><code>torch.Size([1115394]) torch.int64\ntensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51, 43,  1,\n        57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10,  0, 31, 54, 43, 39, 49,\n         6,  1, 57, 54, 43, 39, 49,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47,\n        58, 47, 64, 43, 52, 10,  0, 37, 53, 59])\n</code></pre>"},{"location":"a02_text/#splitting-data-into-train-and-val","title":"Splitting data into train / and val","text":"<p>Will split our long sequence into train + val. </p> <p><code>train</code> will be what the model sees <code>valid</code> will be what is used to see how the model does on new material.</p> <p>The ratio can depend on the size, but <code>80 / 20</code> or <code>90 / 10</code> is pretty common for smaller datasets.</p> <pre><code>train_cutoff = int(0.9 * len(token_ids))\ntrain_data = token_ids[:train_cutoff]\nvali_date = token_ids[valid_cutoff:]\n</code></pre>"},{"location":"a03_dataloaders/","title":"Dataloaders + Datasets","text":"<p>Loading portions of the data a little at a time</p> <pre><code>import torch\nfrom typing import Sequence\ndef read_in_shakespear_data(file: str) -&gt; str:\n\"\"\"returns a long string buffer\"\"\"\nwith open(file, \"r\", encoding=\"utf-8\") as f:\nreturn f.read()\nclass MyDataUtils:\ndef __init__(self, text_file: str):\nbigtext = read_in_shakespear_data(text_file)\nprint(\"total length of characters: {:,}\".format(len(bigtext)))\nself.bigtext = bigtext\nself.unique_characters = sorted(list(set(bigtext)))\nself.vocab_size = len(self.unique_characters)\nprint(\"\".join(self.unique_characters))\nself.encoder = {char: j for j, char in enumerate(self.unique_characters)}\nself.decoder = {j: char for j, char in enumerate(self.unique_characters)}\ndef encode(self, some_str: str) -&gt; Sequence[int]:\nreturn [self.encoder[char] for char in some_str]\ndef decode(self, sequence: Sequence[int]) -&gt; str:\nchars = [self.decoder[num] for num in sequence]\nreturn \"\".join(chars)\ndef get_tokens(self, text: str) -&gt; torch.LongTensor:\ntoken_ids = torch.tensor(\nself.encode(text),\ndtype=torch.long,\n)\nprint(token_ids.shape, token_ids.dtype)\nprint(token_ids[:100])\nreturn token_ids \nmy_data_utils = MyDataUtils(\"./notes/data/input.txt\")\ntoken_ids = my_data_utils.get_tokens(my_data_utils.bigtext)\ntrain_cutoff = int(0.9 * len(token_ids))\ntrain_data = token_ids[: train_cutoff]\nvalid_data = token_ids[train_cutoff:]\n</code></pre> <p>Dumps the following:</p> <pre><code>total length of characters: 1,115,394\n\n !$&amp;',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\ntorch.Size([1115394]) torch.int64\ntensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51, 43,  1,\n        57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10,  0, 31, 54, 43, 39, 49,\n         6,  1, 57, 54, 43, 39, 49,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47,\n        58, 47, 64, 43, 52, 10,  0, 37, 53, 59])\n</code></pre>"},{"location":"a03_dataloaders/#about-dataloads","title":"About dataloads","text":"<p>Often it is useful too look at a subset of a larger sequence</p> <pre><code>block_size = 8\ntoken_ids[:block_size + 1]\n</code></pre> <pre><code>tensor([18, 47, 56, 57, 58,  1, 15, 47, 58])\n</code></pre>"},{"location":"a03_dataloaders/#problem-setup","title":"Problem setup:","text":"<p>A few characters will be provided, and the problem will be the predict what the next character will be. So for example:</p> <p>Input:</p> <pre><code>[18] -&gt; try to predict [47]\n[18, 47] -&gt; try to predict [ 56]\n[18, 47, 56] -&gt; try to predict [ 57]\n[18, 47, 56, 57] -&gt; try to predict [ 58]\n</code></pre> <p>The below code will demonstrate how to do this</p> <pre><code>x = train_data[: block_size]\ny = train_data[1: block_size + 1]\nfor t in range(block_size):\ncontext = x[: t + 1]\ntarget = y[t]\nprint(f\"when input is {context} the target: {target}\")\n</code></pre> <p>And the output looks something like the following:</p> <pre><code>when input is tensor([18]) the target: 47\nwhen input is tensor([18, 47]) the target: 56\nwhen input is tensor([18, 47, 56]) the target: 57\nwhen input is tensor([18, 47, 56, 57]) the target: 58\nwhen input is tensor([18, 47, 56, 57, 58]) the target: 1\nwhen input is tensor([18, 47, 56, 57, 58,  1]) the target: 15\nwhen input is tensor([18, 47, 56, 57, 58,  1, 15]) the target: 47\nwhen input is tensor([18, 47, 56, 57, 58,  1, 15, 47]) the target: 58\n</code></pre>"},{"location":"a03_dataloaders/#using-pytorchs-built-in-classes","title":"Using pytorch's built in classes","text":"<pre><code>import torch\nfrom typing import Tuple\nfrom torch.utils.data import DataLoader, Dataset\nclass MyTorchDataset(Dataset):\ndef __init__(self, window_size: int, long_seq: torch.LongTensor):\nself.seq = long_seq\nself.window_size = window_size\ndef __len__(self):\nreturn self.seq.shape[0] - self.window_size - 1\ndef __getitem__(self, idx) -&gt; Tuple[torch.LongTensor, torch.LongTensor]: \nx = self.seq[idx: idx + self.window_size]\ny = self.seq[idx + 1 : idx + 1 + self.window_size]\nreturn x, y\n</code></pre> <p>As an example if we do the following:</p> <pre><code>batch_size = 4\nblock_size = 8\ntrain_dataset = MyTorchDataset(block_size, train_data)\nvalid_dataset = MyTorchDataset(block_size, valid_data)\ntrain_dataloader = DataLoader(train_dataset, batch_size)\nvalid_dataloader = DataLoader(valid_dataset, batch_size)\nnext(iter(train_dataloader))\n</code></pre> <p>The output from the dataloader is <pre><code>[tensor([[18, 47, 56, 57, 58,  1, 15, 47],\n[47, 56, 57, 58,  1, 15, 47, 58],\n[56, 57, 58,  1, 15, 47, 58, 47],\n[57, 58,  1, 15, 47, 58, 47, 64]]),\ntensor([[47, 56, 57, 58,  1, 15, 47, 58],\n[56, 57, 58,  1, 15, 47, 58, 47],\n[57, 58,  1, 15, 47, 58, 47, 64],\n[58,  1, 15, 47, 58, 47, 64, 43]])]\n</code></pre></p>"},{"location":"a04_first_model/","title":"First Model","text":""},{"location":"a04_first_model/#the-first-bigram-model","title":"The first Bigram model","text":"<pre><code>import torch\nimport torch.nn as nn\nimport numpy as np\nfrom torch.nn import functional as F\ntorch.manual_seed(1337)\nclass BigramLanguageModel(nn.Module):\ndef __init__(self, vocab_size: int):\nsuper().__init__()\n# vocab size square this is almost like attention\n# should note this is considered to be row-wise\nself.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\ndef forward(self, idx, targets  = None):\n# Batch | Time | Channel\n# batch=4, time=8, channel is vocab size=65\nlogits = self.token_embedding_table(idx) # (B, T, C)\n# logits = scores of what will be next in a sequence\n# batch, target, channels\nB, T, C = logits.shape\nif targets is None:\nreturn logits, None\n# unwinding\nlogits = logits.view(B * T, C)\ntargets = targets.view(B * T)\nloss = F.cross_entropy(logits, targets)\nreturn logits, loss\ndef generate(self, idx: torch.LongTensor, max_new_tokens: int):\n\"\"\"\n        Args:\n            idx: a sequence of long ints representing an input\n        \"\"\"\nfor _ in range(max_new_tokens):\n# get the predictions\nlogits, loss = self(idx)\n# focus only on the last time step\nlogits = logits[:, -1, :]\n# get the softmax probabilities\nprobs = F.softmax(logits, dim=-1)\n# sample from the distribtuion (so its not deterministic)\nidx_next = torch.multinomial(probs, num_samples=1)\n# extends the sequence\nidx = torch.cat((idx, idx_next), dim=1) \nreturn idx \n</code></pre> <p>A sample of how to interact with the model</p> <pre><code>m = BigramLanguageModel(my_data_class.vocab_size)\nxb1, yb1 = next(iter(train_dataloader))\nprint(xb1.shape, yb1.shape)\nout, _ = m(xb1, yb1)\nprint(out.shape)  \nexpected_loss = np.log(1/ 65)\nprint(\"average expected loss: {:.04f}\".format(expected_loss))\n</code></pre> <p>The output is the following:</p> <pre><code>torch.Size([4, 8]) torch.Size([4, 8])\ntorch.Size([32, 65])\naverage expected loss: -4.1744\n</code></pre> <p>Trying out the generative method:</p> <pre><code>starter_idx = torch.zeros((1, 1), dtype=torch.long)\npredicted_token_ids = m.generate(starter_idx, max_new_tokens=100)[0].tolist()\noutput = my_data_class.decode(predicted_token_ids)\nprint(\"[untrained model output]: {}\".format(output))\n</code></pre> <p>Note that this model has not been trained on anything, so we expect the output to be terrible</p> <pre><code>[untrained model output]: \no,3foB$MVkzM'Q&amp;!.iFlttRjXKoiw3y?c;iQJaSYI!poBdttRnYC My,WWThP:X3pEH-wNA'zr'XnQhfp\nS-b3fp\njCJyhsm.WUG\n</code></pre>"},{"location":"a04_first_model/#lets-train-the-model","title":"Let's train the model","text":"<pre><code># training\n# training\nm = BigramLanguageModel(my_data_utils.vocab_size)\noptimizer = torch.optim.Adam(m.parameters(), lr=1e-3)\nbatch_size = 32\nlast_loss = 999999\ntrain_dataloader = DataLoader(train_dataset, batch_size)\nfor steps in range(1, 20_000):\nxb, yb = next(iter(train_dataloader))\nlogits, loss = m(xb, yb)\noptimizer.zero_grad(set_to_none=True)\nloss.backward()\noptimizer.step()\nif steps % 500 == 0:\nprint(\"steps: {:,} | {:.04f}\".format(steps, loss.item()))\nimp = (last_loss - loss.item()) / last_loss\nif imp &lt; 0.01:\nprint(\"loss improvement: {}\".format(imp))\nbreak\nelse:\nlast_loss = loss.item()\n</code></pre> <p>The following is the output:</p> <pre><code>steps: 500 | 4.0185\nsteps: 1,000 | 3.1753\nsteps: 1,500 | 2.4656\nsteps: 2,000 | 1.9072\nsteps: 2,500 | 1.5012\nsteps: 3,000 | 1.2300\nsteps: 3,500 | 1.0615\nsteps: 4,000 | 0.9599\nsteps: 4,500 | 0.8977\nsteps: 5,000 | 0.8580\nsteps: 5,500 | 0.8316\nsteps: 6,000 | 0.8134\nsteps: 6,500 | 0.8006\nsteps: 7,000 | 0.7912\nsteps: 7,500 | 0.7843\nloss improvement: 0.00871082619848534\n</code></pre> <p>What happens if we run the generative function now?</p> <pre><code>starter_idx = torch.zeros((1, 1), dtype=torch.long)\npredicted_token_ids = m.generate(starter_idx, max_new_tokens=300)[0].tolist()\noutput = my_data_utils.decode(predicted_token_ids)\nprint(\"[untrained model output]: {}\".format(output))\n</code></pre>"},{"location":"a04_first_model/#sample-generative-output","title":"Sample Generative Output","text":"<pre><code>================================================================================\nGenerative Output!\n================================================================================\n\nWI\nIO IN:\nmine hsthed fal,PXoue.qKLlefot ven!\nAhes D gutse d tenthangowiownn d t h.\nI\nANoushappor:\nS:\nThe UDY uer, ssS:\n' tove:\n\nThPS: t! s:\nmyou sthe gh, INCLTIGw wo cim.'s;cum Qmatland t th yourathayeBul;\nNTh:\nTLI hantin teateer ine she d or ale ELecedist tofol.cowous, areay!CLExcMangNT: answehe womosour me,\nTount:\nZERieatr a coBy!BE?\nKI porngile o ctis u othQUJ3in,\nPrel he;dre.\nTem'stowo n'YFUC&amp;ZIUWBy , LI thendsGZw; blqTMod a inr?\nWhEE nchlos fjeen thisof\nQthe l?\nAr ceYoS! CH-e soAUR:f abHes\n</code></pre>"},{"location":"a05_first_improvement/","title":"First improvement: Using History","text":""},{"location":"a05_first_improvement/#diagnosis-only-looking-at-the-last-character","title":"Diagnosis: only looking at the last character","text":"<ul> <li>one of the reasons the generative model is doing so poorly is it is only looking at the last character (single) to do the prediction. Note the following:</li> </ul> <pre><code>    def generate(\nself,\nidx: torch.LongTensor,\nmax_new_tokens: int\n):\n\"\"\"\n        Args:\n            idx: a sequence of long ints representing an input\n        \"\"\"\nfor _ in range(max_new_tokens):\n# get the predictions\nlogits, loss = self(idx)\n# focus only on the last time step\nlogits = logits[:, -1, :]\n...\n</code></pre>"},{"location":"a05_first_improvement/#when-generating-future-tokens-words","title":"When generating future tokens + words:","text":"<p>When considering a sequence:</p> <pre><code>A-B-C-D-?\n</code></pre> <p>The next letter can use the preceeding tokens as information to help the next item in the sequence. But considering training data:</p> <pre><code>A-B-C-D-[E]-F-G\n</code></pre> <p>Our prediction slot should NOT know the future, in this case <code>F</code> and <code>G</code>.</p>"},{"location":"a05_first_improvement/#averaging-the-history","title":"Averaging the history","text":"<p>What is the easiest way for tokens to communicate to each other?</p> <p>The easiest way to communicate the past is to a average of all the preceding token. So considering the example before</p> <pre><code>A-B-C-D-?\n</code></pre> <p>Can use all the information at <code>D</code> and average the information of <code>A-B-C-D</code> somehow to be useful. The following is how to write it out in pytorch. Consider the following:</p> <pre><code># consider the following example:\ntorch.manual_seed(1337)\nB, T, C = 4, 8, 2 # batch, time, channels\n# generate some random data\nx_dummy = torch.randint(0, 5, size=(B, T, C), dtype=torch.float)\nx_dummy\n</code></pre> <p>The output looks like the following:</p> <pre><code>tensor([[[0., 2.],\n[2., 0.],\n[0., 3.],\n[0., 0.],\n[4., 0.],\n[2., 0.],\n[2., 1.],\n[0., 3.]],\n[[1., 4.],\n[4., 0.],\n[3., 1.],\n[2., 0.],\n[2., 1.],\n[1., 0.],\n[4., 4.],\n[0., 4.]],\n[[1., 4.],\n[4., 4.],\n[4., 3.],\n[4., 3.],\n[4., 3.],\n[3., 0.],\n[4., 3.],\n[3., 3.]],\n[[4., 1.],\n[1., 3.],\n[1., 0.],\n[0., 3.],\n[4., 3.],\n[3., 1.],\n[1., 1.],\n[2., 1.]]])\n</code></pre>"},{"location":"a05_first_improvement/#how-to-calculate-the-average-at-different-timesteps","title":"How to calculate the average at different timesteps","text":"<p>Now consider a small inefficient loop to calculate the mean(s):</p> <pre><code># x bag-of-words, starts empty\n# essentially ignoring the order, the information is important\nxbow = torch.zeros((B, T, C))\nfor b in range(B):\n# for each record in B\nfor t in range(T):\n# for each timestep, get average vectors\n# everything before + including current token\nxprev = x_dummy[b, : t + 1]\nxbow[b, t] = torch.mean(xprev, 0)\n</code></pre> <p>Looking at the output, lets check some of the work:</p> <pre><code>tensor([[[0.0000, 2.0000],\n[1.0000, 1.0000],\n[0.6667, 1.6667],\n[0.5000, 1.2500],\n[1.2000, 1.0000],\n[1.3333, 0.8333],\n[1.4286, 0.8571],\n[1.2500, 1.1250]],\n[[1.0000, 4.0000],\n[2.5000, 2.0000],\n[2.6667, 1.6667],\n[2.5000, 1.2500],\n[2.4000, 1.2000],\n[2.1667, 1.0000],\n[2.4286, 1.4286],\n[2.1250, 1.7500]],\n[[1.0000, 4.0000],\n[2.5000, 4.0000],\n[3.0000, 3.6667],\n[3.2500, 3.5000],\n[3.4000, 3.4000],\n[3.3333, 2.8333],\n[3.4286, 2.8571],\n[3.3750, 2.8750]],\n[[4.0000, 1.0000],\n[2.5000, 2.0000],\n[2.0000, 1.3333],\n[1.5000, 1.7500],\n[2.0000, 2.0000],\n[2.1667, 1.8333],\n[2.0000, 1.7143],\n[2.0000, 1.6250]]])\n</code></pre> <p>Observations:</p> <ol> <li>the first row stays the same, because there's nothing before it in terms of history</li> <li>the decimal complexity further increases farther into the sequence because at step 2, the denomiator is 2, and at step 7, its 7</li> </ol> <p>A quick deep dive:</p> <pre><code># x_dummy\ntensor([[[0., 2.],\n[2., 0.],\n[0., 3.],\n...\n# xbow\ntensor([[[0.0000, 2.0000],\n[1.0000, 1.0000],\n[0.6667, 1.6667], # &lt;---- consider this row\n[0.5000, 1.2500],\n[1.2000, 1.0000],\n[1.3333, 0.8333],\n[1.4286, 0.8571],\n[1.2500, 1.1250]],\n</code></pre>"},{"location":"a05_first_improvement/#making-the-calculation-more-efficient","title":"Making the calculation more efficient","text":"<p>As shown by the above, the loop is very inefficient, lets try and fix this with matrix multiplication</p> <pre><code>import torch\ntorch.manual_seed(42)\na = torch.ones(3, 3)\nb = torch.randint(0, 10, (3, 2), dtype=torch.float)\nc = a @ b\nprint(\"a={}\".format(a))\nprint(\"b={}\".format(b))\nprint(\"c={}\".format(c))\n</code></pre> <pre><code>a=tensor([[1., 1., 1.],\n        [1., 1., 1.],\n        [1., 1., 1.]])\nb=tensor([[2., 7.],\n        [6., 4.],\n        [6., 5.]])\nc=tensor([[14., 16.],\n        [14., 16.],\n        [14., 16.]])\n</code></pre> <p>Note that the differing dimensions of what is being multiplied:</p> <pre><code>[3 x 3] @ [3 x 2] = [3 x 2]\n</code></pre> <p>And this is out the math works</p> <pre><code>14 = 2 x 1 + 6 x 1 + 6 x 1\n16 = 7 x 1 + 4 x 1 + 5 x 1\n</code></pre>"},{"location":"a05_first_improvement/#using-tril-or-lower-triangle","title":"Using <code>tril</code> or <code>lower triangle</code>","text":"<p>This time we will us the <code>lower triangle</code> function or <code>tril</code> to get the following:</p> <pre><code>torch.tril(torch.ones(3, 3))\n</code></pre> <pre><code>a = tensor([[1., 0., 0.],\n[1., 1., 0.],\n[1., 1., 1.]])\nc = a @ b\n</code></pre> <p>The output of <code>a @ c</code> is as follows:</p> <pre><code>tensor([[ 2.,  7.],\n        [ 8., 11.],\n        [14., 16.]])\n</code></pre> <p>The first row is simply a copy, the second row is a rolling sum</p> <pre><code>8 = 2 + 6\n11 = 7 + 4\n</code></pre>"},{"location":"a05_first_improvement/#using-lower-triangle-to-calculate-averages","title":"Using <code>lower triangle</code> to calculate averages","text":"<pre><code>a = torch.tril(torch.ones(3, 3))\n# normalizes the rolling sum\na = a / torch.sum(a, 1, keepdim=True)\n</code></pre> <p>notice that every row sums up to 1. Also think about this as \"evenly spreading out attention\"</p> <pre><code>tensor([[1.0000, 0.0000, 0.0000],\n        [0.5000, 0.5000, 0.0000],\n        [0.3333, 0.3333, 0.3333]])\n</code></pre> <p>Now consider the same <code>a @ c</code>:</p> <pre><code>tensor([[2.0000, 7.0000],\n        [4.0000, 5.5000],\n        [4.6667, 5.3333]])\n</code></pre> <p>The first row only has 1 set of values to look at, so its the same vals</p> <pre><code>2 = 2 x 1.\n7 = 7 x 1.\n\n4 = 2 x 0.5 + 6 x 0.5\n5.5 = 7 x 0.5 + 4 x 0.5\n</code></pre>"},{"location":"a05_first_improvement/#back-to-weight-calculation","title":"Back to weight calculation","text":"<p>Lets setup our averaging matrix for a larger size <code>T = 8</code></p> <pre><code>wei = torch.tril(torch.ones(T, T))\nwei = wei / wei.sum(1, keepdim=True)\nwei\n</code></pre> <pre><code>tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n[0.5000, 0.5000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n[0.3333, 0.3333, 0.3333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n[0.2500, 0.2500, 0.2500, 0.2500, 0.0000, 0.0000, 0.0000, 0.0000],\n[0.2000, 0.2000, 0.2000, 0.2000, 0.2000, 0.0000, 0.0000, 0.0000],\n[0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.0000, 0.0000],\n[0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.0000],\n[0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250]])\n</code></pre> <p>Using the above matrix to do rolling averages:</p> <pre><code># (T, T) @ (B, T, C) --&gt; (B, T, C)\n# will apply this by batch [(T, T) @ (T, C)]\nxbow2 = wei @ xdummy\n# compare the two\ntorch.allclose(xbow, xbow2)\n# &gt;&gt;&gt; true\n</code></pre>"},{"location":"a05_first_improvement/#using-softmax-v3-for-calculating-average-history","title":"Using Softmax V3 for calculating average history","text":"<pre><code>## Softmax approach\ntril = torch.tril(torch.ones(T, T))\nwei = torch.zeros((T, T))\n# using the triangle as a max\nwei = wei.masked_fill(tril == 0, float('-inf'))\n# take a softmax along every dimension\n# softmax is a normalization operation\n# softmax exponentiates each of the values + normalizes\n# exp(0) = 1, exp(-inf) = 0\nwei = F.softmax(wei, dim=-1)\n</code></pre> <pre><code>tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n[0.5000, 0.5000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n[0.3333, 0.3333, 0.3333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n[0.2500, 0.2500, 0.2500, 0.2500, 0.0000, 0.0000, 0.0000, 0.0000],\n[0.2000, 0.2000, 0.2000, 0.2000, 0.2000, 0.0000, 0.0000, 0.0000],\n[0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.0000, 0.0000],\n[0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.0000],\n[0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250]])\n</code></pre>"},{"location":"a06_upgrading_the_model/","title":"Adding some changes","text":""},{"location":"a06_upgrading_the_model/#1-adding-an-embedding-dimension","title":"1. Adding an embedding dimension","text":"<p>Previously</p> <pre><code>self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n</code></pre> <p>Paraphase: storing the general likelihood of next character. So given <code>A</code>, whats the chance of the other characters?</p> <p>Now:</p> <pre><code>self.token_embedding_table = nn.Embedding(vocab_size, n_emb)\nself.lm_head = nn.Linear(n_embed, vocab_size)\n</code></pre> <p>Paraphrase: Each letter has some data related to it <code>n_emb</code> float numbers. Then some combination of those numbers will be multiplied down to the frequencies of the next character</p> <p>To accommodate this, need to make changes in the <code>forward</code> function as well:</p> <pre><code>token_emb = self.token_embedding_table(idx) # (B, T, C)\nlogits = self.lm_head(tok_emb)  # (B, T, vocab_size)\n</code></pre>"},{"location":"a06_upgrading_the_model/#2-encode-the-position","title":"2. Encode the position","text":"<pre><code>self.position_embedding_table = nn.Embedding(block_size, n_emb)\n</code></pre> <p>and in the <code>forward</code> part of the function</p> <pre><code>pos_emb = self.position_embedding_table(torch.arange(T, device=device))\nx = token_emb + pos_emb\n</code></pre> <pre><code>import torch\nimport torch.nn as nn\nimport numpy as np\nfrom torch.nn import functional as F\ntorch.manual_seed(1337)\nclass BigramLanguageModel(nn.Module):\ndef __init__(self, vocab_size: int, n_emb: int = 32):\nsuper().__init__()\n# vocab size square this is almost like attention\n# should note this is considered to be row-wise\nself.token_embedding_table = nn.Embedding(vocab_size, n_emb)\n# encoding where the character is in the overall\nself.position_embedding_table = nn.Embedding(block_size, n_emb)\nself.lm_head = nn.Linear(n_embed, vocab_size)\ndef forward(self, idx, targets = None):\nB, T = idx.shape\n# Batch | Time | Channel\n# batch=4, time=8, channel is NOW the embed\n# get the token embedding\ntoken_emb = self.token_embedding_table(idx) # (B, T, C)\n# get the position embedding\npos_emb = self.position_embedding_table(torch.arange(T, device=device))\nx = token_emb + pos_emb\nlogits = self.lm_head(x)  # (B, T, vocab_size)\n# logits = scores of what will be next in a sequence\n# batch, target, channels\nB, T, C = logits.shape\nif targets is None:\nreturn logits, None\n# unwinding\nlogits = logits.view(B * T, C)\ntargets = targets.view(B * T)\nloss = F.cross_entropy(logits, targets)\nreturn logits, loss\ndef generate(self, idx: torch.LongTensor, max_new_tokens: int):\n\"\"\"\n        Args:\n            idx: a sequence of long ints representing an input\n        \"\"\"\nfor _ in range(max_new_tokens):\n# get the predictions\nlogits, loss = self(idx)\n# focus only on the last time step\nlogits = logits[:, -1, :]\n# get the softmax probabilities\nprobs = F.softmax(logits, dim=-1)\n# sample from the distribtuion (so its not deterministic)\nidx_next = torch.multinomial(probs, num_samples=1)\n# extends the sequence\nidx = torch.cat((idx, idx_next), dim=1) \nreturn idx \n</code></pre>"},{"location":"a07_position_embedding/","title":"WTF is self-attention","text":"<p>Remember from before:</p> <pre><code>## Softmax approach\ntril = torch.tril(torch.ones(T, T))\nwei = torch.zeros((T, T))\n# using the triangle as a max\nwei = wei.masked_fill(tril == 0, float('-inf'))\n# take a softmax along every dimension\n# softmax is a normalization operation\n# softmax exponentiates each of the values + normalizes\n# exp(0) = 1, exp(-inf) = 0\nwei = F.softmax(wei, dim=-1)\n</code></pre> <p>The above calculation results in an array like below:</p> <pre><code>tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n[0.5000, 0.5000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n[0.3333, 0.3333, 0.3333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n[0.2500, 0.2500, 0.2500, 0.2500, 0.0000, 0.0000, 0.0000, 0.0000],\n[0.2000, 0.2000, 0.2000, 0.2000, 0.2000, 0.0000, 0.0000, 0.0000],\n[0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.0000, 0.0000],\n[0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.0000],\n[0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250]])\n</code></pre>"},{"location":"a07_position_embedding/#think-about-a-sentence-some-words-are-more-important-than-others","title":"Think about a sentence, some words are more important than others","text":"<pre><code>\"From the market, I purchased some apples\"\n</code></pre> <p>The more imporant words in the above sentence is <code>market</code> and <code>apples</code>.</p> <p>While averaging is not a bad idea, most times, we don't want uniform consideration.</p>"},{"location":"a07_position_embedding/#how-does-self-attention-solve-this","title":"How does self-attention solve this?","text":"<p>Every item in a sequence will emit 2 vectors:</p> <p><code>Query</code> - \"the query vector\", what am i looking for <code>Key</code> - \"what do i contain\"</p> <p>So how do the different items between the keys + the querys get affinity?</p> <p><code>Query [dot product] keys of all other tokens</code>, and that result will be come <code>wei</code></p> <p>If the affinity is high, the dot product result should be high.</p>"},{"location":"a07_position_embedding/#how-to-implement","title":"How to implement","text":"<p>Implement a single <code>head</code> of self-attention. There's something in this field called <code>head size</code></p> <pre><code>torch.manual_seed(1337)\nB, T, C = 4, 8, 32 # batch , time, embedding channels\nx = torch.randn(B, T, C)\n# making a single head of self-attention\nhead_size = 16\n# note that these values will be \"learned\" and are not constant\nkey = nn.Linear(C, head_size, bias=False)  # (B, T, 16)\nquery = nn.Linear(C, head_size, bias=False)  # (B, T, 16)\n# so whats created here is a Key + Query for each of the inputs\n# no communication has happened yet\nk = key(x)  # (B, T, 16)\nq = query(x)  # (B, T, 16)\n# this wei will represent the affinities between the different possibilities\nwei = q @ k.transpose(-2, -1)  # (B, T, 16) @ (B, 16, T) = (B, T, T)\n# the above linear layers lets non-uniform emphasis to be made\n# the below still enables the moving history\ntril = torch.tril(torch.ones(T, T))\nwei = wei.masked_fill(tril == 0, float(\"-inf\"))\nwei = F.softmax(wei, dim=-1)\nout = wei @ x\n</code></pre> <p>If the non-uniform <code>wei</code> is examined:</p> <pre><code>torch.set_printoptions(linewidth=200, threshold=100_000, sci_mode=False)\n</code></pre> <p>Should note that the results are not uniform anymore</p> <pre><code>tensor([[[1.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00],\n         [0.16, 0.84, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00],\n         [0.21, 0.16, 0.63, 0.00, 0.00, 0.00, 0.00, 0.00],\n         [0.58, 0.12, 0.19, 0.11, 0.00, 0.00, 0.00, 0.00],\n         [0.03, 0.11, 0.05, 0.03, 0.79, 0.00, 0.00, 0.00],\n         [0.02, 0.27, 0.02, 0.01, 0.68, 0.00, 0.00, 0.00],\n         [0.17, 0.41, 0.04, 0.04, 0.10, 0.20, 0.03, 0.00],\n         [0.02, 0.08, 0.06, 0.23, 0.06, 0.07, 0.24, 0.24]]\n</code></pre>"},{"location":"a07_position_embedding/#what-about-value","title":"What about <code>value</code>?","text":"<p>So from above, there's a learnable distribution of \"what external information might be useful?\"</p> <p>But note, that is not the same thing as retrieving the actual value. Collecting those \"outside\" values is established by the following:</p> <pre><code>torch.manual_seed(1337)\nB, T, C = 4, 8, 32 # batch , time, embedding channels\nx = torch.randn(B, T, C)\n# making a single head of self-attention\nhead_size = 16\nkey = nn.Linear(C, head_size, bias=False)  # (B, T, 16)\nquery = nn.Linear(C, head_size, bias=False)  # (B, T, 16)\n# adding layer\nvalue = nn.Linear(C, head_size, bias=False)  # (B, T, 16)\nk = key(x)  # (B, T, 16)\nq = query(x)  # (B, T, 16)\n# this wei will represent the affinities between the different possibilities\nwei = q @ k.transpose(-2, -1)  # (B, T, 16) @ (B, 16, T) = (B, T, T)\n# the above linear layers lets non-uniform emphasis to be made\n# the below still enables the moving history\ntril = torch.tril(torch.ones(T, T))\nwei = wei.masked_fill(tril == 0, float(\"-inf\"))\nwei = F.softmax(wei, dim=-1)\n# the other elements we aggregate\nv = value(x)\nout = wei @ v\n</code></pre> <p>So consider <code>x</code> as <code>private</code> information about this token. </p> <p>So if im the 5th item in a series. </p> <ul> <li>my private information is in <code>x</code></li> <li>im interested in info stored in <code>q</code></li> <li>i am offering information in <code>k</code></li> <li>actual value that ill return in <code>v</code></li> </ul>"},{"location":"a07_position_embedding/#comments-on-attention-positional-encoding","title":"Comments on Attention + Positional encoding","text":"<p>Attention is a communication mechanism, almost like a network. And attention gives a certain weight to information connected to our current location. A quick rehash:</p> <pre><code>A-&gt;B-&gt;C-&gt;D-&gt;E\n\nA is connected to nothing (but itself)\nB is connected to A\nC is connected to B and A, etc.\nAnd E is connected to all the previously letters\n</code></pre> <p>The the issue with the above is for letter <code>E</code>, there's no concept of \"space\", there's no difference to the location of A + B, it just knows that its connected to it.</p>"},{"location":"a07_position_embedding/#the-batch-dimension-is-isolated","title":"The batch dimension is isolated","text":"<p>Everything in the batch dimension should be isolated. Data across the batch dimension should not interact with one another.</p>"},{"location":"a07_position_embedding/#what-if-all-the-tokens-need-to-talk-to-one-another","title":"what if all the tokens need to talk to one another?","text":"<ul> <li>Sentiment analysis</li> <li>fill in the blank (need to look before + after)</li> <li>Encoder block of self-attention, run the above, but remove this one line     <pre><code>wei = wei.masked_fill(tril == 0, float(\"-inf\"))\n</code></pre></li> </ul>"},{"location":"a07_position_embedding/#what-is-cross-attention","title":"What is cross attention","text":"<p>What we have described here is <code>self-attention</code> because the keys, queries, and values come from the same data. but they don't have to come from the same section</p> <ul> <li>Encoder &lt;&gt; Decoders example:<ul> <li>the queries are produced from X, but keys + values are produced from a separate section</li> <li>when there's separate source of nodes</li> </ul> </li> </ul>"},{"location":"a07_position_embedding/#dividing-by-the-headsize-aka-scaled-attention","title":"Dividing by the headsize AKA scaled attention","text":"<p>The reason for denominator value (from the paper) is the following:</p> <ul> <li>softmax tends to go to one-hot encoding at extreme positive and negative values</li> <li>so dividing by the number of heads will curb some of that to ensure the calculated distribution is not so extreme</li> </ul>"},{"location":"a08_model_with_attention/","title":"Desigin attention head","text":"<pre><code>import torch\nimport torch.nn as nn\nimport numpy as np\nfrom torch.nn import functional as F\ntorch.manual_seed(1337)\nclass Head(nn.Module):\n\"\"\"one head of attention\"\"\"\ndef __init__(self, head_size: int, n_emb: int, block_size: int):\nsuper().__init__()\n# linear layers\nself.key = nn.Linear(n_emb, head_size, bias=False)\nself.query = nn.Linear(n_emb, head_size, bias=False)\nself.value = nn.Linear(n_emb, head_size, bias=False)\n# not a param of the module\n# this is a buffer, assigning to module\nself.register_buffer(\n\"tril\",\ntorch.tril(torch.ones(block_size, block_size))\n)\ndef forward(self, x):\nB, T, C = x.shape\nk = self.key(x)\nq = self.query(x)\n# attention\nwei = q @ k.transpose(-2, -1) * C **-0.5\nwei = wei.masked_fill(self.tril[:T, :T] == 0, float(\"-inf\"))\nwei = F.softmax(wei, dim=-1)  #(B, T, T)\n# perform the weighted aggregation of the values\nv = self.value(x)\nout = wei @ v \nreturn out\n</code></pre> <p>now that <code>Head</code> is created we can integrate it with the bigram model</p> <pre><code>import torch\nimport torch.nn as nn\nimport numpy as np\nfrom torch.nn import functional as F\ntorch.manual_seed(1337)\nclass BigramLanguageModel(nn.Module):\ndef __init__(self, vocab_size: int, n_emb: int = 32, block_size: int):\nsuper().__init__()\n# vocab size square this is almost like attention\n# should note this is considered to be row-wise\nself.token_embedding_table = nn.Embedding(vocab_size, n_emb)\nself.block_size = block_size\n# encoding where the character is in the overall\nself.position_embedding_table = nn.Embedding(block_size, n_emb)\nself.sa_head = Head(head_size=n_emb, n_emb=n_emb, block_size=block_size)\nself.lm_head = nn.Linear(n_embed, vocab_size)\ndef forward(self, idx, targets = None):\nB, T = idx.shape\n# get the token embedding\ntoken_emb = self.token_embedding_table(idx) # (B, T, C)\n# get the position embedding\npos_emb = self.position_embedding_table(torch.arange(T, device=device))\nx = token_emb + pos_emb\nx = self.sa_head(x)  # apply one head of self-attention\nlogits = self.lm_head(x)  # (B, T, vocab_size)\n# logits = scores of what will be next in a sequence\n# batch, target, channels\nB, T, C = logits.shape\nif targets is None:\nreturn logits, None\n# unwinding\nlogits = logits.view(B * T, C)\ntargets = targets.view(B * T)\nloss = F.cross_entropy(logits, targets)\nreturn logits, loss\ndef generate(self, idx: torch.LongTensor, max_new_tokens: int):\n\"\"\"\n        Args:\n            idx: a sequence of long ints representing an input\n        \"\"\"\nfor _ in range(max_new_tokens):\n# crop IDx to the last block size tokens\nidx_cond = idx[:, -self.block+size:]\n# get the predictions\nlogits, loss = self(idx_cond)\n# focus only on the last time step\nlogits = logits[:, -1, :]\n# get the softmax probabilities\nprobs = F.softmax(logits, dim=-1)\n# sample from the distribtuion (so its not deterministic)\nidx_next = torch.multinomial(probs, num_samples=1)\n# extends the sequence\nidx = torch.cat((idx, idx_next), dim=1) \nreturn idx \n</code></pre> <p>Once the new model has been trained, we we can generate the following output:</p> <pre><code>starter_idx = torch.zeros((1, 1), dtype=torch.long)\npredicted_token_ids = m.generate(starter_idx, max_new_tokens=300)[0].tolist()\noutput = my_data_utils.decode(predicted_token_ids)\nprint(\"[trained model output]: {}\".format(output))\n</code></pre> <pre><code>python -m notes.b08b_train --gpu\n</code></pre> <pre><code>When bef bridcowr, har on, ble\n\nHiset bube dienavegr-ans mealatanss:\nWant he uw hathe.\nWar dilasoate awice my.\n\nHastaru zorou tabuts, tof is hy me mil; dill, aes iree sen cin lat Herid ovets, and Win ngar iserans!\nel lind peal.\n-hul wonchiry ptupr aiss hew ye wllinde norod atelaves\nMomy ys, dl tthake on indo whe Ceiiby, wisti dourive wees ime st so mowrixs bure kad nterupt f so;\nARID Wam:\nEN CI inle ont ffaf Pre?\n\nWh om.\n\nHe-fu a!\nAtied isar adsal the Eand thinin cour ay aney Iry ts I fr yo ves,\n</code></pre>"},{"location":"a09_multihead_attention/","title":"What is multi-head attention?","text":"<p>Training multiple heads of attention in parallel, so we will create an abstraction that allows for multiple heads.</p> <pre><code>class MultiHeadAttention(nn.Module):\ndef __init__(self, num_heads: int, *args, **kwargs):\nsuper().__init__()\nself.heads = nn.ModuleList([Head(*args, **kwargs)])\ndef forward(self, x):\nreturn torch.cat(\n[h(x) for h in self.heads],\ndim=-1\n)\n</code></pre> <p>This will add more throughput to the processing. Can look at attention in detail.</p> <p>In the paper a feed-forward layer is added to give the model more time to think:</p> <pre><code>class FeedForward(nn.Module):\ndef __init__(self, n_emb: int):\nsuper().__init__()\nself.net = nn.Sequential(\nnn.Linear(n_emb, n_emb),\nnn.ReLU(),\n)\ndef forward(self, x):\nreturn self.net(x)\n</code></pre> <p>Sample generative output</p> <pre><code>And ther thidcow and is and the airen bobe to: anvirtand me?\nI ands ar hiphe us hat vet?\nF dilth ane aw crup and not com onour\nYowns\nMoof is her!\nAmil;\nAnd buaes ifee-sen cin lat Heriviov the and Wing.\n\nDWAFNWIO:\nWel lind teall thus wonchiry:\nAur Maiss hawty.\n\nKIBEUSIR:\nI patelives\nMom\nmy wod mothake on indo wher eiicks withe dourive wies ime st so mower; the the danderupt for ar igis! muf thin inled\nAtatfif Pried my of.\n\nHINY IER:\nWidby ake adsal ther ghe thidin cour ay andy Iry to chan the!\nAy\n</code></pre>"},{"location":"a10_multiple_layers/","title":"Adding multiple layers","text":"<p>https://youtu.be/kCc8FmEb1nY?t=5201</p>"},{"location":"a10_multiple_layers/#we-want-to-stack-multiple-attention-feedforward-processing","title":"We want to stack multiple Attention + Feedforward processing","text":"<pre><code>import torch\nfrom torch import nn\nclass AttnFFBlock(nn.Module)::\ndef __init__(self, n_emb: int, n_head: int):\nsuper().__init__()\nhead_size = n_embed // n_head\nself.sa = MultiHeadAttention(n_head, head_size)\nself.ffwd = FeedForward(n_emb)\ndef forward(self, x):\nx = self.sa(x)\nx = self.ffwd(x)\nreturn x\n</code></pre> <p>And in the model, this will look like the following:</p> <pre><code>...\nself.token_embedding_table = nn.Embedding(...)\nself.position_embedding_table = nn.Embedding(...)\nself.blocks = nn.Sequential(\nBlock(n_embed, n_head=4),\nBlock(n_embed, n_head=4),\nBlock(n_embed, n_head=4)\n)\n</code></pre> <p>This will help a lot, but also at this point we will be building a \"deeper\" network. The dangerous thing about this is that the deeper networks can get more unstable. To address this, we will implement some additional techniques.</p>"},{"location":"a10_multiple_layers/#residual-connections","title":"Residual Connections","text":"<p>This idea was taken from RESNET. The main idea is there is a clear pathway that skips data manipulation. </p> <ul> <li>in the beginning, these residual pathways are initialized do not adjust the input</li> <li>but as training progresses, the residual blocks come online + start learning</li> <li>this allows for much deeper networks</li> <li>this helps with stability of model training</li> </ul> <p>Implementation</p> <pre><code>class AttnFFBlock(nn.Module):\ndef __init__(self, n_emb: int, n_head: int):\nsuper().__init__()\nhead_size = n_emb // n_head\nself.sa = MultiHeadAttention(n_head, head_size)\nself.ffwd = FeedForward(n_emb)\ndef forward(self, x):\nx = x + self.sa(x)\nx = x + self.ffwd(x)\nreturn x\n</code></pre> <p>The multi-head attention must be updated too</p> <pre><code>class MultiHeadAttention(nn.Module):\ndef __init__(self, num_heads: int, n_emb, *args, **kwargs):\nsuper().__init__()\nself.heads = nn.ModuleList([Head(*args, **kwargs)])\nself.proj = nn.Linear(n_emb, n_emb)\ndef forward(self, x):\nout = torch.cat(\n[h(x) for h in self.heads],\ndim=-1\n)\nout = self.proj(out)\nreturn out\n</code></pre> <p>The feed forward will also need to be updated.</p> <ul> <li>according to the paper, the embedding dim should be 4x</li> <li>A projection layer will be added</li> </ul> <pre><code>class FeedForward(nn.Module):\ndef __init__(self, n_emb: int):\nsuper().__init__()\nself.net = nn.Sequential(\nnn.Linear(n_emb, 4 * n_emb),\nnn.ReLU(),\nnn.Linear(4 * n_emb, n_emb), # projection layer\n)\ndef forward(self, x):\nreturn self.net(x)\n</code></pre> <p>Here's a sample of the output after applying the above</p> <pre><code>total runtime: 0:01:56.509885\n================================================================================\nGenerative Output!\n================================================================================\n\nWARCIO:\nRoricch.\n\nSOROLOLE:\nAUn a sel bube to take O--\nSere?\n\nTause:\nWarth fuur crown.\nWhre that a endwick, you, not to zurm he ourservefuin her than well\nWhines is enten cintlatise, drove the dener there is wacs!\n lilind than.\n-huld cuch by prave aiss hinty.\n\nKING HI mandpere a dest my liked mothad\nTo Wichon her evicks the most rive ches,\nWe sto-LARGANGELO:\nThake my cructed so;\nAnd hine:\nEdward.\n\nPUENTIZ:\nFirse?\n\nKIS\nBetrang, be!\n\nHOMPEY:\n\nSadUpl the Eursend uin cour aar tey ir-prave from clocy\n</code></pre>"},{"location":"a11_layernorm/","title":"Layer Norm","text":""},{"location":"a11_layernorm/#layer-norm-and-batch-norm","title":"Layer Norm and Batch Norm","text":""},{"location":"a11_layernorm/#batch-normalization","title":"Batch Normalization","text":"<p>Across the batch dimension, any individual neuron has unit gaussian distribution, meaning:</p> <ul> <li>mean is 0</li> <li>stdev is 1</li> </ul> <p>A quick example (will use pre-built)</p> <pre><code>import torch\nfrom torch import nn\nmodz = nn.BatchNorm1d(num_features=100)\nx = torch.randn(32, 100) # batch size of 32 x 100 dim vecs\nx = modz(x)\nx.shape, x[0, :].mean(), x[0, :].std(), x[1, :].mean(), x[1, :].std()\n</code></pre> <pre><code>(torch.Size([32, 100]),\n tensor(0.0474, grad_fn=&lt;MeanBackward0&gt;),\n tensor(1.0037, grad_fn=&lt;StdBackward0&gt;),\n tensor(0.0294, grad_fn=&lt;MeanBackward0&gt;),\n tensor(1.1358, grad_fn=&lt;StdBackward0&gt;))\n</code></pre> <p>Layer Norm</p> <pre><code>import torch\nclass BatchNorm1d:\ndef __init__(self, dim, eps=1e-5, momentum=0.1):\nself.eps = eps\nself.gamma = torch.ones(dim)\nself.beta = torch.zeros(dim)\nself.out = None\ndef __call__(self, x: torch.Tensor):\n# forward pass (this will be looking row-wise)\nxmean = x.mean(1, keepdim=True) # get the batch mean\nxvar = x.var(1, keepdim=True) # batch variance\nxhat = (x - xmean) / torch.sqrt(xvar + self.eps)  # normalize to unit variance\nself.out = self.gamma * xhat + self.beta\nreturn self.out\ndef parameters(self):\nreturn [self.gamma, self.beta]\n</code></pre> <p>Now that we have implemented this. Lets add it in:</p> <ul> <li>general comment: very few transformer details have changed over the years</li> <li>the orignal paper, layer norm is applied after the transformation through the attention head</li> <li>in current times its more common to apply the norm BEFORE the attention is applied</li> </ul> <pre><code>class AttnFFBlock(nn.Module):\ndef __init__(self, n_emb: int, n_head: int):\nsuper().__init__()\nhead_size = n_emb // n_head\nself.sa = MultiHeadAttention(n_head, n_emb, head_size)\nself.ffwd = FeedForward(n_emb)\nself.ln1 = nn.LayerNorm(n_emb)  # adding layer norm, has trainable\nself.ln2 = nn.LayerNorm(n_emb)  # adding layer norm, has trainable\ndef forward(self, x):\nx = x + self.sa(self.ln1(x))  # normalize the features\nx = x + self.ffwd(self.ln2(x))  # normalize the features\nreturn x\n</code></pre> <p>Will add layernorm after the block stack, in the Lagnague model</p> <pre><code>        self.blocks = nn.Sequential(\nAttnFFBlock(n_emb, n_head=4),\nAttnFFBlock(n_emb, n_head=4),\nAttnFFBlock(n_emb, n_head=4),\nnn.LayerNorm(n_emb) # adding here\n)\nself.lm_head = nn.Linear(n_emb, vocab_size)\n</code></pre>"},{"location":"a11_layernorm/#dropout","title":"Dropout","text":"<p>Takes neural network, drops out certain weights so the model will not memorize the data. Makes it more robust and essentially adds regularization</p>"},{"location":"a11_layernorm/#note-that-the-training-at-this-point-will-be-significantly-slower","title":"Note that the training at this point will be SIGNIFICANTLY SLOWER","text":"<ul> <li>There's a slow startup time (it will hang)</li> <li>Then afterwards, i swapped to a GPU to do the training</li> </ul> <p>Here's the output after predicting:</p> <pre><code>total runtime: 0:07:49.969858\n================================================================================\nGenerative Output!\n================================================================================\n\nKING RICHARD II:\nShall be stir a senators again:\nBrief Mercutio\nMy arm that us all to become as a funward,\nAnd follow me to heaven from the heart more.\nYou, beg it end, I heard it to move thee.\n\nWARWICK:\nAy, just, leave you well us wonder yourself.\n\nKING HENRY VI:\nHe came at last your mistress than\nAnd your white is your maids, and hie my means,--\n\nKING RICHARD III:\nFrom your present! my lord, I leave,\nYour Prince, Signio; I speak too, is a royal thing:\nFor this county, and your battle may lose\n</code></pre>"},{"location":"a12_optimized/","title":"Optimized Outputs","text":"<p>https://www.youtube.com/watch?v=kCc8FmEb1nY&amp;t=6081s</p> <pre><code>batch_size = 64\nblock_size = 256\nmax_iters = 5000\neval_interval = 500\nlearning_rate = 3e-4 \ndevice = \"cuda\"\neval_iters = 200\nn_emb = 384\nn_head = 6\nn_layer = 6\ndropout = 0.2 \n</code></pre> <ul> <li>larger batch size</li> <li>256 characters of context to predict</li> <li>learning rate is reduced, because the NN is much larger</li> <li>many more heads , at 6 heads, its 384 / 6 = 64 dim as standards</li> <li>6 layers</li> <li>0.2 - every forward and backwards pass loses 20%</li> </ul> <p>Running on a Nvidia 3090, sample output</p> <pre><code>python -m notes.b12b_train --gpu\n================================================================================\nusing : cuda\n================================================================================\n================================================================================\nmodel is on cuda!\n================================================================================\n  0%|                                                                                                   | 0/5000 [00:00&lt;?, ?it/s]iter: 0 | train_loss: 4.4755    | valid_loss: 4.4713 cycle: 0:00:00.001323 total_time: 0:00:00.001325\n 10%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589                                                                                | 500/5000 [01:44&lt;09:44,  7.70it/s]iter: 500       | train_loss: 2.0300    | valid_loss: 2.1061 cycle: 0:01:04.726900 total_time: 0:01:44.400129\n 20%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c                                                                      | 1000/5000 [03:29&lt;08:58,  7.42it/s]iter: 1,000     | train_loss: 1.6249    | valid_loss: 1.7993 cycle: 0:01:05.124363 total_time: 0:03:29.321880\n 30%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d                                                             | 1500/5000 [05:14&lt;07:31,  7.75it/s]iter: 1,500     | train_loss: 1.4493    | valid_loss: 1.6673 cycle: 0:01:04.658950 total_time: 0:05:14.450527\n 40%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f                                                    | 2000/5000 [06:59&lt;06:25,  7.77it/s]^[[Citer: 2,000 | train_loss: 1.3459    | valid_loss: 1.5890 cycle: 0:01:04.577669 total_time: 0:06:59.165923\n 50%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588                                            | 2500/5000 [08:45&lt;05:25,  7.69it/s]iter: 2,500     | train_loss: 1.2720    | valid_loss: 1.5538 cycle: 0:01:05.944031 total_time: 0:08:45.437820\n 59%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e                                   | 2973/5000 [10:26&lt;04:21,  7.75it/s] 60%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a                                   | 3000/5000 [10:30&lt;04:25,  7.53it/s]iter: 3,000     | train_loss: 1.2085    | valid_loss: 1.5571 cycle: 0:01:04.693232 total_time: 0:10:30.205539\n 70%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c                          | 3500/5000 [12:16&lt;03:13,  7.74it/s]iter: 3,500     | train_loss: 1.1388    | valid_loss: 1.5596 cycle: 0:01:05.024825 total_time: 0:12:16.216906\n 80%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d                 | 4000/5000 [14:01&lt;02:09,  7.73it/s]iter: 4,000     | train_loss: 1.0722    | valid_loss: 1.6034 cycle: 0:01:05.704443 total_time: 0:14:01.813210\n 90%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f        | 4500/5000 [15:49&lt;01:04,  7.72it/s]iter: 4,500     | train_loss: 0.9874    | valid_loss: 1.6838 cycle: 0:01:06.705413 total_time: 0:15:49.758759\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 4999/5000 [17:35&lt;00:00,  7.51it/s]iter: 4,999     | train_loss: 0.8881    | valid_loss: 1.8121 cycle: 0:01:05.292599 total_time: 0:17:35.475523\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 5000/5000 [18:16&lt;00:00,  4.56it/s]\ntotal runtime: 0:18:16.412797\n================================================================================\nGenerative Output!\n================================================================================\n\nAnd this bright face of base King Edward's death.\n\nOXFORD:\nSlanders, and pity what I bury, didst the sword,\nAnd, not with true towns at for Edward's life;\nWhich, if the new orish Hereford the king?\n\nTYBALT:\nNo, my glad of Hereford, my Lord Northumberland,\nWhen some Henry Sixth God John Lycome Northumbol,--\nAnd the king of Somerset, Each of Warwick!\n3 KING HENRY VI\n\nYORK:\nWhat's this, Warwick, I know against Margaret,\nThe rages of Cretervance and devil Edward.\n\nYORK:\nMaster, your high-hand follow\n(base) root@DESKTOP-605N4AP:/home/tlee/myrepos/course-chat-gpt# \n</code></pre>"},{"location":"a13_encoder_decoder/","title":"About Encoder Decoders","text":"<p>Everything that we have done is concerning <code>decoder</code></p> <p>The key difference here is the following comparison:</p> <ul> <li>the Queries come from the generative start (x)</li> <li>the Keys + and th Values come from the encoder network</li> <li>this is cross attention, in that all the phrases are available</li> <li>so when generating input, it is based on the past + the full availability of input</li> <li>this is called <code>conditioning</code></li> </ul>"},{"location":"a13_encoder_decoder/#nano-gpt","title":"Nano GPT","text":"<p><code>Training</code> will be much differenc e because of the following:</p> <ul> <li>adding checkpoints during model training</li> <li>training on multi-gpu</li> <li>pretrained weights</li> <li>decaying the learning rate</li> <li>more options</li> </ul> <p>The <code>model</code> should be very similar.</p> <ul> <li>What is different is the Multi-head attention. Instead of separating out two heads, make a 4D array instead.</li> <li>GeLU</li> </ul>"},{"location":"a13_encoder_decoder/#chatgpt","title":"ChatGPT","text":"<p>What if we wanted to train ourselves?</p> <ul> <li><code>Pretraining</code> stage: learn from the internet, and train it to \"babble\" about the internet. Massive infrastructure challenge. 1000's of GPUs to train this.<ul> <li>this trains a document completer. It will create articles + documents, trying to complete the sequence.</li> <li>undefined behavior, try to complete some news article</li> </ul> </li> <li><code>2ndstage</code> condition on assistance<ul> <li>Part A: documents: <code>question + answer</code>, on the order of thousands of examples. Fine tune the model on documents like this. Sample efficient on fine-tuning</li> <li>Part B: Let the model respond in different ways, than raters will select which answer they prefer, then will use this as a <code>reward</code> piece, predict using a different network, to guess which response would be preferred</li> <li>Part C: then run <code>PPO</code> policy gradient reinforcement, to fine tune the answers, are expected to score a high reward. There is a whole fine tuning stage.</li> <li>Much harder to replicate this stage</li> </ul> </li> </ul>"}]}